{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-enforcement",
   "metadata": {},
   "source": [
    "# Reimplementing the Adversarially Reweighted Learning model by Lahoti et al. (2020) to improve fairness without demographics\n",
    "\n",
    "\n",
    "\n",
    "This notebook contains the results presented in the paper by J. Mohazzab, L. Weytingh, C. Wortmann, and B. Brocades Zaalberg. More specifically, it contains the presented results for replicating [the paper by Lahoti et al.](https://arxiv.org/abs/2006.13114). In addition, this notebook includes the significance tests presented in Section 3.4.1 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distant-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import train\n",
    "from argparser import DefaultArguments, get_optimal_parameters\n",
    "from significance import test_significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-stanley",
   "metadata": {},
   "source": [
    "### Default Parameters\n",
    "\n",
    "The default parameters are loaded below. They can be changed, e.g. for speeding up the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default parameters are:\n",
      " {'average_over': 10, 'dataset': 'compas', 'train_steps': 1000, 'pretrain_steps': 250, 'batch_size': 32, 'optimizer': 'Adagrad', 'embedding_size': 32, 'lr_learner': 0.01, 'lr_adversary': 0.01, 'test_every': 5, 'seed': 42, 'log_dir': 'logs/', 'res_dir': 'results/', 'print_loss': False, 'model_name': 'ARL'}\n"
     ]
    }
   ],
   "source": [
    "# Load the default arguments\n",
    "default_args = DefaultArguments()\n",
    "\n",
    "# Change if the loss should be printed\n",
    "default_args.print_loss = False\n",
    "\n",
    "# Change the amount of times the results are averaged here.\n",
    "default_args.average_over = 10\n",
    "\n",
    "# Change the amount of training steps for each of the datasets here.\n",
    "training_steps = {\n",
    "    \"uci_adult\": 990,\n",
    "    \"law_school\": 990,\n",
    "    \"compas\": 470,\n",
    "}\n",
    "\n",
    "print(\"The default parameters are:\\n\", default_args.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-albany",
   "metadata": {},
   "source": [
    "## Replicability\n",
    "\n",
    "The presented results for the PyTorch implementation are generated below for each classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-cisco",
   "metadata": {},
   "source": [
    "### Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "green-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters used for the Adult dataset:\n",
      " {'batch_size': 256, 'lr_learner': 0.01, 'lr_adversary': 1, 'train_steps': 990}\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal hyperparameters.\n",
    "adult_params = get_optimal_parameters(\"uci_adult\")\n",
    "adult_params[\"train_steps\"] = training_steps[\"uci_adult\"]\n",
    "\n",
    "# Load the arguments passed to the training function.\n",
    "adult_args = copy.copy(default_args)\n",
    "adult_args.dataset = \"uci_adult\"\n",
    "adult_args.update(adult_params)\n",
    "\n",
    "print(\"Parameters used for the Adult dataset:\\n\", adult_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "large-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.904 ± 0.0020\n",
      "Average AUC(macro-avg): 0.914\n",
      "Average AUC(min): 0.878\n",
      "Average AUC(minority): 0.949\n",
      "-----------------------------------\n",
      "\n",
      "Training and evaluating took, on average, 42 seconds per model iteration for Adult\n"
     ]
    }
   ],
   "source": [
    "# Start timing.\n",
    "adult_start = time.time()\n",
    "\n",
    "# Train the model.\n",
    "train.main(adult_args)\n",
    "\n",
    "# Save the timing results.\n",
    "adult_time = (time.time() - adult_start) / adult_args.average_over\n",
    "print(f\"Training and evaluating took, on average, {adult_time:.0f} seconds per model iteration for Adult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-encyclopedia",
   "metadata": {},
   "source": [
    "### LSAC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sonic-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters used for the LSAC dataset:\n",
      " {'batch_size': 256, 'lr_learner': 0.1, 'lr_adversary': 0.01, 'train_steps': 990}\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal hyperparameters.\n",
    "lsac_params = get_optimal_parameters(\"law_school\")\n",
    "lsac_params[\"train_steps\"] = training_steps[\"law_school\"]\n",
    "\n",
    "# Load the arguments passed to the training function.\n",
    "lsac_args = copy.copy(default_args)\n",
    "lsac_args.dataset = \"law_school\"\n",
    "lsac_args.update(lsac_params)\n",
    "\n",
    "print(\"Parameters used for the LSAC dataset:\\n\", lsac_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crucial-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.820 ± 0.0091\n",
      "Average AUC(macro-avg): 0.817\n",
      "Average AUC(min): 0.795\n",
      "Average AUC(minority): 0.829\n",
      "-----------------------------------\n",
      "\n",
      "Training and evaluating took, on average, 25 seconds per model iteration for LSAC\n"
     ]
    }
   ],
   "source": [
    "# Start timing.\n",
    "lsac_start = time.time()\n",
    "\n",
    "# Train the model.\n",
    "train.main(lsac_args)\n",
    "\n",
    "# Save the timing results.\n",
    "lsac_time = (time.time() - lsac_start) / lsac_args.average_over\n",
    "print(f\"Training and evaluating took, on average, {lsac_time:.0f} seconds per model iteration for LSAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-matthew",
   "metadata": {},
   "source": [
    "### COMPAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expected-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters used for the COMPAS dataset:\n",
      " {'batch_size': 32, 'lr_learner': 0.01, 'lr_adversary': 1, 'train_steps': 470}\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal hyperparameters.\n",
    "compas_params = get_optimal_parameters(\"compas\")\n",
    "compas_params[\"train_steps\"] = training_steps[\"compas\"]\n",
    "\n",
    "# Load the arguments passed to the training function.\n",
    "compas_args = copy.copy(default_args)\n",
    "compas_args.dataset = \"compas\"\n",
    "compas_args.update(compas_params)\n",
    "\n",
    "print(\"Parameters used for the COMPAS dataset:\\n\", compas_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excellent-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.721 ± 0.0065\n",
      "Average AUC(macro-avg): 0.702\n",
      "Average AUC(min): 0.616\n",
      "Average AUC(minority): 0.754\n",
      "-----------------------------------\n",
      "\n",
      "Training took, on average, 9 seconds per model iteration for COMPAS\n"
     ]
    }
   ],
   "source": [
    "# Start timing.\n",
    "compas_start = time.time()\n",
    "\n",
    "# Train the model.\n",
    "train.main(compas_args)\n",
    "\n",
    "# Save the timing results.\n",
    "compas_time = (time.time() - compas_start) / compas_args.average_over\n",
    "print(f\"Training took, on average, {compas_time:.0f} seconds per model iteration for COMPAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-keeping",
   "metadata": {},
   "source": [
    "### Average runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "approximate-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average runtime per model iteration is 25 seconds.\n"
     ]
    }
   ],
   "source": [
    "avg_runtime = sum([adult_time, lsac_time, compas_time]) / 3\n",
    "print(f\"The average runtime per model iteration is {avg_runtime:.0f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-setup",
   "metadata": {},
   "source": [
    "## Significance testing\n",
    "\n",
    "The significance of the results of the ARL model are tested against a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-mustang",
   "metadata": {},
   "source": [
    "### Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constitutional-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.904 ± 0.0021\n",
      "Average AUC(macro-avg): 0.913\n",
      "Average AUC(min): 0.877\n",
      "Average AUC(minority): 0.948\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model without Adversary.\n",
    "adult_args.model_name = \"baseline\" \n",
    "train.main(adult_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stone-reggae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value for uci_adult is 0.963\n",
      "This difference is not significant\n"
     ]
    }
   ],
   "source": [
    "# Test the significance.\n",
    "test_significance(\"uci_adult\", adult_args.res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-surface",
   "metadata": {},
   "source": [
    "### LSAC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powerful-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.820 ± 0.0075\n",
      "Average AUC(macro-avg): 0.818\n",
      "Average AUC(min): 0.798\n",
      "Average AUC(minority): 0.834\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model without Adversary\n",
    "lsac_args.model_name = \"baseline\"\n",
    "train.main(lsac_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "urban-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value for law_school is 0.977\n",
      "This difference is not significant\n"
     ]
    }
   ],
   "source": [
    "# Test the significance.\n",
    "test_significance(\"law_school\", lsac_args.res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-transmission",
   "metadata": {},
   "source": [
    "### COMPAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominant-application",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n",
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n",
      "Training model 5/10\n",
      "Training model 6/10\n",
      "Training model 7/10\n",
      "Training model 8/10\n",
      "Training model 9/10\n",
      "Training model 10/10\n",
      "Done training\n",
      "\n",
      "-----------------------------------\n",
      "Results\n",
      "\n",
      "Average AUC: 0.721 ± 0.0072\n",
      "Average AUC(macro-avg): 0.702\n",
      "Average AUC(min): 0.624\n",
      "Average AUC(minority): 0.748\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model without Adversary\n",
    "compas_args.model_name = \"baseline\"\n",
    "train.main(compas_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spectacular-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value for compas is 0.977\n",
      "This difference is not significant\n"
     ]
    }
   ],
   "source": [
    "# Test the significance.\n",
    "test_significance(\"compas\", compas_args.res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-value",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
